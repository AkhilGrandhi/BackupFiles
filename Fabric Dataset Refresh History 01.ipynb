{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2aa9283-dab8-4967-bd17-754c222d7a1b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T04:56:16.0208088Z",
       "execution_start_time": "2025-03-10T04:56:15.6780175Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e2e8e326-495f-400a-a596-abd1e557d685",
       "queued_time": "2025-03-10T04:56:15.4740685Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import time\n",
    "from pyspark.sql.functions import col\n",
    "import msal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c0caa-e812-40ea-92f9-4cdf8cb39fbb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T04:56:16.5548889Z",
       "execution_start_time": "2025-03-10T04:56:16.1951667Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f516bfec-f411-4ff5-814b-3d7aa80706a0",
       "queued_time": "2025-03-10T04:56:15.9495761Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Azure AD Credentials (Replace with your values)\n",
    "TENANT_ID = \"\"\n",
    "CLIENT_ID = \"\"\n",
    "CLIENT_SECRET = \"\"\n",
    "\n",
    "# Azure AD Authority & Scope\n",
    "# AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n",
    "# SCOPE = [\"https://api.fabric.microsoft.com/.default\"]  # Fabric API Scope\n",
    "# TOKEN_URL = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
    "\n",
    "AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n",
    "SCOPE = [\"https://analysis.windows.net/powerbi/api/.default\"]  # Power BI Scpoe\n",
    "TOKEN_URL = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/token\"\n",
    "\n",
    "\n",
    "\n",
    "# Global variables for token caching\n",
    "ACCESS_TOKEN = None\n",
    "EXPIRATION_TIME = 0  # Store expiration time\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"Fetch and cache an access token from Azure AD for Power BI API.\"\"\"\n",
    "    global ACCESS_TOKEN, EXPIRATION_TIME\n",
    "\n",
    "    # Check if token is still valid\n",
    "    if ACCESS_TOKEN and time.time() < EXPIRATION_TIME:\n",
    "        return ACCESS_TOKEN  # Return cached token if still valid\n",
    "\n",
    "    # Create MSAL client application\n",
    "    app = msal.ConfidentialClientApplication(CLIENT_ID, authority=AUTHORITY, client_credential=CLIENT_SECRET)\n",
    "\n",
    "    # Acquire token\n",
    "    token_response = app.acquire_token_for_client(scopes=SCOPE)\n",
    "\n",
    "    if \"access_token\" in token_response:\n",
    "        ACCESS_TOKEN = token_response[\"access_token\"]\n",
    "        EXPIRATION_TIME = time.time() + token_response.get(\"expires_in\", 3600) - 300  # Refresh 5 minutes before expiry\n",
    "        return ACCESS_TOKEN\n",
    "    else:\n",
    "        raise Exception(f\"Failed to get token: {token_response}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc34538-b08b-4fa6-9ac4-50555a1017ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "session_starting",
       "parent_msg_id": "5d9b904e-3438-43ac-8bf8-0896cbbd3fbb",
       "queued_time": "2025-03-12T08:24:29.3251723Z",
       "session_id": null,
       "session_start_time": "2025-03-12T08:24:29.3271878Z",
       "spark_pool": null,
       "state": "session_starting",
       "statement_id": -1,
       "statement_ids": []
      },
      "text/plain": [
       "StatementMeta(, , -1, SessionStarting, , SessionStarting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reading the Datafrom the delta file\n",
    "workspacess_df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"abfss://d3120490-76ae-4ef4-a440-2bd65732ccdc@onelake.dfs.fabric.microsoft.com/fecab367-5d3a-41c1-8037-7801192932ba/Tables/fabric_workpsaces\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1499b542-a5d2-473d-8796-98e2d2bfe1a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T04:56:40.8174134Z",
       "execution_start_time": "2025-03-10T04:56:32.1485357Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d6904d46-cb6e-4e0a-b60e-9a21bdeea1ef",
       "queued_time": "2025-03-10T04:56:16.5701012Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Filter Workspaces for a capactity\n",
    "filtered_workspaces_df = workspacess_df.filter(col(\"capacityId\") == \"76F4499E-05FF-44A9-8C2F-323EE14EA1A7\")\n",
    "workspaceIds_list = [row[\"id\"] for row in filtered_workspaces_df.select(\"id\").collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc68601f-cc1c-40e7-9ffb-73f5952dce81",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T04:56:41.9252299Z",
       "execution_start_time": "2025-03-10T04:56:40.9881942Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "651c1072-f87b-49be-b674-67f42c5fb903",
       "queued_time": "2025-03-10T04:56:16.7165475Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reading the Datafrom the delta file\n",
    "datasets_df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"abfss://d3120490-76ae-4ef4-a440-2bd65732ccdc@onelake.dfs.fabric.microsoft.com/fecab367-5d3a-41c1-8037-7801192932ba/Tables/datasets\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ae5ff8-8296-4601-8236-922a0721d3b9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T04:56:46.0105459Z",
       "execution_start_time": "2025-03-10T04:56:42.1077143Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "69403434-da6b-4bf7-aa2c-99ef72851e15",
       "queued_time": "2025-03-10T04:56:17.3099864Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Filter Datsets for a capactity\n",
    "filtered_datasets_df = datasets_df.filter(col(\"workspaceId\").isin(workspaceIds_list))\n",
    "datasetIds_list = [row[\"id\"] for row in filtered_datasets_df.select(\"id\").collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53726890-dd62-47b4-8560-3cb91b2b7797",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T05:07:42.9461196Z",
       "execution_start_time": "2025-03-10T05:07:42.607113Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8471e440-8286-4cae-97e6-2dc461c14c5d",
       "queued_time": "2025-03-10T05:07:42.444527Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get_dataset_refresh_histroy\n",
    "def get_dataset_refresh_histroy(workspaceId, datasetId):\n",
    "    \"\"\"Fetch dataset refresh history from Power BI API.\"\"\"\n",
    "    token = get_access_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    errors = []\n",
    "    url = f\"https://api.powerbi.com/v1.0/myorg/groups/{workspaceId}/datasets/{datasetId}/refreshes\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        errors.append(pd.DataFrame([{\"datasetId\": datasetId, \"statusCode\":response.status_code} | {key: None for key in [\n",
    "            \"requestId\", \"id\", \"refreshType\", \"status\", \"refreshAttempts\",\n",
    "            \"startTime\", \"endTime\", \"DataStartTime\", \"DataEndTime\", \n",
    "            \"QueryStartTime\", \"QueryEndTime\"]}]))\n",
    "\n",
    "        return (pd.DataFrame([{\"datasetId\": datasetId} | {key: None for key in [\n",
    "            \"requestId\", \"id\", \"refreshType\", \"status\", \"refreshAttempts\",\n",
    "            \"startTime\", \"endTime\", \"DataStartTime\", \"DataEndTime\", \n",
    "            \"QueryStartTime\", \"QueryEndTime\"]}]), errors)\n",
    "\n",
    "    res = response.json()\n",
    "    if not res.get(\"value\"):  # If \"value\" is empty or missing, return default None values\n",
    "        errors.append(pd.DataFrame([{\"datasetId\": datasetId, \"statusCode\":response.status_code} | {key: None for key in [\n",
    "            \"requestId\", \"id\", \"refreshType\", \"status\", \"refreshAttempts\",\n",
    "            \"startTime\", \"endTime\", \"DataStartTime\", \"DataEndTime\", \n",
    "            \"QueryStartTime\", \"QueryEndTime\"]}]))\n",
    "        return (pd.DataFrame([{\"datasetId\": datasetId} | {key: None for key in [\n",
    "            \"requestId\", \"id\", \"refreshType\", \"status\", \"refreshAttempts\",\n",
    "            \"startTime\", \"endTime\", \"DataStartTime\", \"DataEndTime\", \n",
    "            \"QueryStartTime\", \"QueryEndTime\"]}]), errors)\n",
    "\n",
    "    records = [{\n",
    "        \"datasetId\": datasetId,\n",
    "        \"requestId\": entry.get(\"requestId\", None),\n",
    "        \"id\": entry.get(\"id\", None),\n",
    "        \"refreshType\": entry.get(\"refreshType\", None),\n",
    "        \"status\": entry.get(\"status\", None),\n",
    "        \"refreshAttempts\": len(entry.get(\"refreshAttempts\", [])),\n",
    "        \"startTime\": entry.get(\"startTime\", None),\n",
    "        \"endTime\": entry.get(\"endTime\", None),\n",
    "        \"DataStartTime\": next((att.get(\"startTime\") for att in entry.get(\"refreshAttempts\", []) if att.get(\"type\") == \"Data\"), None),\n",
    "        \"DataEndTime\": next((att.get(\"endTime\") for att in entry.get(\"refreshAttempts\", []) if att.get(\"type\") == \"Data\"), None),\n",
    "        \"QueryStartTime\": next((att.get(\"startTime\") for att in entry.get(\"refreshAttempts\", []) if att.get(\"type\") == \"Query\"), None),\n",
    "        \"QueryEndTime\": next((att.get(\"endTime\") for att in entry.get(\"refreshAttempts\", []) if att.get(\"type\") == \"Query\"), None)\n",
    "    } for entry in res[\"value\"]]\n",
    "\n",
    "    return (pd.DataFrame(records),errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f92f1a-00df-43ed-8884-c379e7bff99b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T05:08:04.9378167Z",
       "execution_start_time": "2025-03-10T05:07:45.090689Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "26c5f9a7-dc67-4475-b377-22285e4bbb45",
       "queued_time": "2025-03-10T05:07:44.918825Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 17,
       "statement_ids": [
        17
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 17, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_temp_dfs = []  # List to store non-empty DataFrames\n",
    "\n",
    "for row in filtered_datasets_df.toLocalIterator():  # No need for unpacking\n",
    "    temp_df, errors = get_dataset_refresh_histroy(row[\"workspaceId\"], row[\"id\"])\n",
    "    \n",
    "    if not temp_df.empty:  # Append only if temp_df is not empty\n",
    "        all_temp_dfs.append(temp_df)\n",
    "    else:\n",
    "        print(f\"No data for: workspaceId={row['workspaceId']}, id={row['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98900c4b-444a-4ac4-8981-fe53e5013783",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T05:11:18.2878267Z",
       "execution_start_time": "2025-03-10T05:10:58.5469455Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "acf2f975-3efa-47b5-94b6-4c480114cee4",
       "queued_time": "2025-03-10T05:10:58.3671401Z",
       "session_id": "1b46249c-5f8b-4775-8df1-a14b161ed0af",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 19,
       "statement_ids": [
        19
       ]
      },
      "text/plain": [
       "StatementMeta(, 1b46249c-5f8b-4775-8df1-a14b161ed0af, 19, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 46 new records to datasets_refresh_history.\n"
     ]
    }
   ],
   "source": [
    "datasets_refresh_history_df = pd.concat(all_temp_dfs, ignore_index=True)\n",
    "#Creating Spark DataFrame\n",
    "spark_datasets_refresh_history_df = spark.createDataFrame(datasets_refresh_history_df)\n",
    "\n",
    "# Read existing requestIds from datasets_refresh_history table\n",
    "#existing_df = spark.read.format(\"delta\").load(\"abfss://d3120490-76ae-4ef4-a440-2bd65732ccdc@onelake.dfs.fabric.microsoft.com/fecab367-5d3a-41c1-8037-7801192932ba/Tables/datasets_refresh_history\").select(\"requestId\")\n",
    "\n",
    "existing_df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"abfss://d3120490-76ae-4ef4-a440-2bd65732ccdc@onelake.dfs.fabric.microsoft.com/fecab367-5d3a-41c1-8037-7801192932ba/Tables/datasets_refresh_history\")\n",
    "    .select(\"requestId\")\n",
    "    .dropna(subset=[\"requestId\"])  # Remove null requestId\n",
    "    .dropDuplicates([\"requestId\"])  # Remove duplicates\n",
    ")\n",
    "\n",
    "new_spark_datasets_refresh_history_df = spark_datasets_refresh_history_df.join(existing_df, \"requestId\", \"left_anti\")\n",
    "\n",
    "if new_spark_datasets_refresh_history_df.count() > 0:\n",
    "    new_spark_datasets_refresh_history_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"datasets_refresh_history\")\n",
    "    print(f\"Appended {new_spark_datasets_refresh_history_df.count()} new records to datasets_refresh_history.\")\n",
    "else:\n",
    "    print(\"No new records found. Skipping append.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582a064-3c98-47ac-b6d8-085a3dcf4f4b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-10T04:11:46.7519222Z",
       "execution_start_time": "2025-03-10T04:11:39.8794331Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "48c5bbdc-36ba-4957-91b6-1d74f86ada28",
       "queued_time": "2025-03-10T04:11:08.605674Z",
       "session_id": "f56a0605-a805-41d4-92bd-b17518eb2276",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 30,
       "statement_ids": [
        30
       ]
      },
      "text/plain": [
       "StatementMeta(, f56a0605-a805-41d4-92bd-b17518eb2276, 30, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows removed, and table updated.\n"
     ]
    }
   ],
   "source": [
    "# Load the Delta table\n",
    "df = spark.read.format(\"delta\").load(\"abfss://d3120490-76ae-4ef4-a440-2bd65732ccdc@onelake.dfs.fabric.microsoft.com/fecab367-5d3a-41c1-8037-7801192932ba/Tables/datasets_refresh_history\")\n",
    "\n",
    "# Remove duplicate rows based on all columns\n",
    "df_deduplicated = df.dropDuplicates()\n",
    "\n",
    "# Overwrite the table with deduplicated data\n",
    "df_deduplicated.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://d3120490-76ae-4ef4-a440-2bd65732ccdc@onelake.dfs.fabric.microsoft.com/fecab367-5d3a-41c1-8037-7801192932ba/Tables/datasets_refresh_history\")\n",
    "\n",
    "print(\"Duplicate rows removed, and table updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c29f59-8ecd-4117-8745-89b76e326e2a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "fecab367-5d3a-41c1-8037-7801192932ba",
    "default_lakehouse_name": "Fabric",
    "default_lakehouse_workspace_id": "d3120490-76ae-4ef4-a440-2bd65732ccdc",
    "known_lakehouses": [
     {
      "id": "fecab367-5d3a-41c1-8037-7801192932ba"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
