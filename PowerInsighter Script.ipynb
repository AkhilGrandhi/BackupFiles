{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import openpyxl\n",
    "import subprocess\n",
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Environment : Public\n",
      "TenantId    : 92431631-d203-4da0-b62c-6fd3e495252f\n",
      "UserName    : akhil.grandhi@dunboxed.com\n",
      "\n",
      "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Inp4ZWcyV09OcFRrd041R21lWWN1VGR0QzZKMCIsImtpZCI6Inp4ZWcyV09OcFRrd041R21lWWN1VGR0QzZKMCJ9.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.AZapTNXDYSCmkVuOFtvGKglVIuDfw3Q9QT7J9F7rvDPCRuJqoBBoT8r2T9RwllXuRb8UokZBghOu-VSF4MN1DbRoLqjvjtYweZwbyL0j95fQuHOFWgtGWCNX7T7wUzl0CoAHqNm42KiM1e2PhKjNhAs3wSH187SQ89jlSCYOI8jbOyVkbKOE217pIut1ceMQkY6_QchusDtcAHlvq2V2bkhoxyAe78Knf1JoFQJBOdNxl1Y0b045i05uaekFWaF9Lr8iutb-WGggeEdwqhAKjJcFRgrus0zBNNXl-xu6x9eq3gzX_VzLwjdtIGRTGRIHqsMd0NXRyMuT85RjaW1Bpw\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Barrier Token Generator\n",
    "ps_script_name = 'PSBarrierToken.ps1'\n",
    "result = subprocess.run(['powershell.exe', './' + ps_script_name], capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = \"Bearer \" + output.split(\".com\")[1][2:-3]\n",
    "headers={'Authorization': Token,\n",
    "'Content-Type':'application/json'}\n",
    "body={\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the Paths with PROD Name Manually we are hard coding the Workspace Names Example: _Adhoc_Model_Prod\n",
    "#Update the repo path to pull changes to a a partciular folder\n",
    "#Use this folder while pulling data from GIT\n",
    "#Place your clone link in the repo_url\n",
    "#Errors Array\n",
    "Errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled the latest changes.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository if it doesn't exist, or fetch the latest changes if it already exists\n",
    "repo_path = \"C://Users//AkhilGrandhi//Downloads//PBIADO/Testing\" # Specify the local path where you want to clone the repository\n",
    "repo_url = \"https://akhilgrandhiTesting@dev.azure.com/akhilgrandhiTesting/Testing/_git/Testing\"  # Specify the Git repository URL\n",
    "\n",
    "try:\n",
    "    # If the repository already exists, fetch the latest changes\n",
    "    repo = git.Repo(repo_path)\n",
    "    origin = repo.remote(name=\"origin\")\n",
    "    origin.fetch()\n",
    "    origin.pull()\n",
    "\n",
    "    print(\"Successfully pulled the latest changes.\")\n",
    "except git.exc.NoSuchPathError:\n",
    "    # If the repository doesn't exist, clone it\n",
    "    repo = git.Repo.clone_from(repo_url, repo_path)\n",
    "\n",
    "    print(\"Successfully cloned the repository.\")\n",
    "except git.exc.GitCommandError as e:\n",
    "    Errors.append([\"An error occurred while pulling the latest changes:\"+e])\n",
    "    print(f\"An error occurred while pulling the latest changes: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the dataset details\n",
    "url=\"https://api.powerbi.com/v1.0/myorg/groups\"\n",
    "dataset_response = requests.get(url,headers=headers)\n",
    "if dataset_response.status_code == 200:\n",
    "    jsonstr = dataset_response.content\n",
    "    workspaces = json.loads(jsonstr)\n",
    "    # List to store data source that needs to be ignored as invalid data sources\n",
    "    workspacesData = workspaces['value']\n",
    "    workspaceIDs = []\n",
    "    isReadOnly = []\n",
    "    isOnDedicatedCapacity=[]\n",
    "    capacityId = [] \n",
    "    defaultDatasetStorageFormat=[]\n",
    "    workspaceType=[]\n",
    "    workspaceNames = []\n",
    "\n",
    "    # requiredWorkspaceNames = ['']\n",
    "    \n",
    "    for workspace in workspacesData:\n",
    "        if('id' in workspace):\n",
    "            workspaceIDs.append(workspace['id'])\n",
    "        if('isReadOnly' in workspace):\n",
    "            isReadOnly.append(workspace['isReadOnly'])\n",
    "        else:\n",
    "            isReadOnly.append('NA')\n",
    "        if('isOnDedicatedCapacity' in workspace):\n",
    "            isOnDedicatedCapacity.append(workspace['isOnDedicatedCapacity'])\n",
    "        else:\n",
    "            isOnDedicatedCapacity.append('NA')\n",
    "        if( 'capacityId' in workspace):\n",
    "            capacityId.append(workspace['capacityId'])\n",
    "        else:\n",
    "            capacityId.append('NA')\n",
    "        if( 'name' in workspace):\n",
    "            workspaceNames.append(workspace['name'])\n",
    "        else:\n",
    "            workspaceNames.append('NA')\n",
    "        if( 'defaultDatasetStorageFormat' in workspace):\n",
    "            defaultDatasetStorageFormat.append(workspace['defaultDatasetStorageFormat'])\n",
    "        else:\n",
    "            defaultDatasetStorageFormat.append('NA')\n",
    "        if( 'type' in workspace):\n",
    "            workspaceType.append(workspace['type'])\n",
    "        else:\n",
    "            workspaceType.append('NA')\n",
    "\n",
    "    tempDict = {'WorkspaceID':workspaceIDs,'IsReadOnly':isReadOnly,'IsOnDedicatedCapacity':isOnDedicatedCapacity,'CapacityID':capacityId,\n",
    "                'DefaultDatasetStorageFormat':defaultDatasetStorageFormat,'WorkspaceType':workspaceType,'WorkspaceName': workspaceNames,}\n",
    "    df_workspace_names_ids = pd.DataFrame(tempDict)\n",
    "else:\n",
    "    Errors.append([\"Error at Workspaces_Data\",dataset_response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workspaces data excel creation\n",
    "df_workspace_names_ids.to_excel(\"Workspaces_Data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stroing All Sematic model details\n",
    "df_Workspace_Datasets = pd.DataFrame()\n",
    "WorkspaceID = []\n",
    "WorkspaceNames = []\n",
    "DatasetIds = []\n",
    "DatasetNames = []\n",
    "DatasetWebUrl = []\n",
    "ConfiguredBy = []\n",
    "CreatedDate = []\n",
    "IsRefreshable = []\n",
    "queryScaleOutSettings = []\n",
    "maxReadOnlyReplicas = []\n",
    "autoSyncReadOnlyReplicas = []\n",
    "\n",
    "\n",
    "for Id in range(len(workspaceIDs)):\n",
    "    workspaceUrl = \"https://api.powerbi.com/v1.0/myorg/groups/\"+workspaceIDs[Id]+\"/datasets\"\n",
    "    Workspace_Datasets_response = requests.get(workspaceUrl,headers=headers)\n",
    "    if Workspace_Datasets_response.status_code == 200:\n",
    "        DatasetJsonstr = Workspace_Datasets_response.content\n",
    "        datasets = json.loads(DatasetJsonstr)\n",
    "        datasetsData = datasets['value']\n",
    "        for dataset in datasetsData:\n",
    "            WorkspaceID.append(workspaceIDs[Id])\n",
    "            WorkspaceNames.append(workspaceNames[Id])\n",
    "            DatasetIds.append(dataset['id'])\n",
    "            DatasetNames.append(dataset['name'])\n",
    "            DatasetWebUrl.append(dataset['webUrl'])\n",
    "            try:\n",
    "                ConfiguredBy.append(dataset['configuredBy']) \n",
    "            except:\n",
    "                ConfiguredBy.append('Microsoft')\n",
    "            CreatedDate.append(dataset['createdDate'])\n",
    "            IsRefreshable.append(dataset['isRefreshable'])\n",
    "            if(dataset['queryScaleOutSettings']):\n",
    "                queryScaleOutSettings.append(True)\n",
    "                autoSyncReadOnlyReplicas.append(dataset['queryScaleOutSettings']['autoSyncReadOnlyReplicas'])\n",
    "                maxReadOnlyReplicas.append(dataset['queryScaleOutSettings']['maxReadOnlyReplicas'])\n",
    "            else:\n",
    "                queryScaleOutSettings.append(False)\n",
    "                autoSyncReadOnlyReplicas.append(dataset['queryScaleOutSettings']['autoSyncReadOnlyReplicas'])\n",
    "                maxReadOnlyReplicas.append(dataset['queryScaleOutSettings']['maxReadOnlyReplicas'])\n",
    "    else:\n",
    "        Errors.append([\"Error at Workspaces_Datasets in \"+workspaceIDs[Id],Workspace_Datasets_response])\n",
    "\n",
    "            \n",
    "tempDict = {'WorkspaceID':WorkspaceID,'WorkspaceName': WorkspaceNames,'DatasetName': DatasetNames,\n",
    "                    'DatasetID':DatasetIds,'DatasetWebUrl':DatasetWebUrl, 'ConfiguredBy':ConfiguredBy,'CreatedDate':CreatedDate, \n",
    "                    'IsRefreshable':IsRefreshable,'QueryScaleOutSettings':queryScaleOutSettings,\n",
    "                    'AutoSyncReadOnlyReplicas':autoSyncReadOnlyReplicas,'MaxReadOnlyReplicas':maxReadOnlyReplicas}\n",
    "df_Workspace_Datasets = pd.DataFrame(tempDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workspace Datasets excel creation\n",
    "df_Workspace_Datasets.to_excel(\"Workspaces_Datasets.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Soures of a dataset\n",
    "DatasourceType = []\n",
    "Server_Path = []\n",
    "Database_Kind = []\n",
    "DatasourceId = []\n",
    "GatewayId = []\n",
    "DatasetID = []\n",
    "WorkspaceID = []\n",
    "\n",
    "\n",
    "for index, row in df_Workspace_Datasets.iterrows():\n",
    "    sourceUrl = \"https://api.powerbi.com/v1.0/myorg/groups/\"+row['WorkspaceID']+\"/datasets/\"+row['DatasetID']+\"/datasources\"\n",
    "    Dataset_SourceDetails_response = requests.get(sourceUrl,headers=headers)\n",
    "    \n",
    "    if Dataset_SourceDetails_response.status_code == 200:\n",
    "        sourceJsonstr = Dataset_SourceDetails_response.content\n",
    "        source = json.loads(sourceJsonstr)\n",
    "        sourceData = source['value']\n",
    "        #print(sourceData)\n",
    "\n",
    "        DatasetID.append(row['DatasetID'])\n",
    "        WorkspaceID.append(row['WorkspaceID'])\n",
    "        try:\n",
    "            DatasourceType.append(sourceData[0]['datasourceType'])\n",
    "        except:\n",
    "            DatasourceType.append(\"NA\")\n",
    "        #connectionDetails\n",
    "        try:\n",
    "            if(\"path\" in sourceData[0]['connectionDetails'] and \"kind\" in sourceData[0]['connectionDetails']):\n",
    "                Server_Path.append(sourceData[0]['connectionDetails']['path'])\n",
    "                Database_Kind.append(sourceData[0]['connectionDetails']['kind'])\n",
    "            elif(\"server\" in sourceData[0]['connectionDetails'] and \"database\" in sourceData[0]['connectionDetails']):\n",
    "                Server_Path.append(sourceData[0]['connectionDetails']['server'])\n",
    "                Database_Kind.append(sourceData[0]['connectionDetails']['database'])\n",
    "            elif(\"url\" in sourceData[0]['connectionDetails']):\n",
    "                Server_Path.append(sourceData[0]['connectionDetails']['url'])\n",
    "                Database_Kind.append(\"NA\")\n",
    "            else:\n",
    "                # print(sourceData[0]['connectionDetails'])\n",
    "                Server_Path.append(sourceData[0]['connectionDetails'])\n",
    "                Database_Kind.append(\"NA\")\n",
    "        except:\n",
    "            Server_Path.append(\"NA\")\n",
    "            Database_Kind.append(\"NA\")\n",
    "        #datasourceId\n",
    "        try:\n",
    "            DatasourceId.append(sourceData[0]['datasourceId'])\n",
    "        except:\n",
    "            DatasourceId.append(\"NA\")\n",
    "        #gatewayId\n",
    "        try:\n",
    "            GatewayId.append(sourceData[0]['gatewayId'])\n",
    "        except:\n",
    "            GatewayId.append(\"NA\")\n",
    "    else:\n",
    "        # print(row['DatasetID'],row['DatasetName'])\n",
    "        Errors.append([\"Error at Dataset_SourceDetails in \"+row['DatasetID']+\" \"+ row['DatasetName'],Dataset_SourceDetails_response])\n",
    "\n",
    "\n",
    "sourceDict = {'WorkspaceID': WorkspaceID, 'DatasetID': DatasetID, 'DatasourceType': DatasourceType, \n",
    "            'Server_Path':Server_Path , 'Database_Kind': Database_Kind, 'DatasourceID': DatasourceId, 'GatewayID': GatewayId}\n",
    "df_sourceDetails = pd.DataFrame(sourceDict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Error at Dataset_SourceDetails in 3563efcb-ffe1-4369-b4bb-467d4d47c48d Feature Usage and Adoption',\n",
       "  <Response [404]>],\n",
       " ['Error at Dataset_SourceDetails in b07832c6-31c9-4153-825e-207685400d5f Purview Hub',\n",
       "  <Response [404]>]]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workspace Datasets excel creation\n",
    "df_sourceDetails.to_excel(\"Dataset_SourceDetails.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets Refresh Details\n",
    "DatasetID = []\n",
    "Frequency = []\n",
    "Days = []\n",
    "Times = []\n",
    "Enabled = []\n",
    "LocalTimeZoneId = []\n",
    "NotifyOption = []\n",
    "\n",
    "\n",
    "for index, row in df_Workspace_Datasets.iterrows():\n",
    "    try:\n",
    "        refreshUrl = \"https://api.powerbi.com/v1.0/myorg/datasets/\"+row['DatasetID']+\"/refreshSchedule\"\n",
    "        refreshUrl_response = requests.get(refreshUrl,headers=headers)\n",
    "        if refreshUrl_response.status_code == 200:\n",
    "            refreshJsonstr = requests.get(refreshUrl,headers=headers).content\n",
    "            refreshData = json.loads(refreshJsonstr)\n",
    "        else:\n",
    "            # print(\"else case:\",row['DatasetId'])\n",
    "            refreshUrl = \"https://api.powerbi.com/v1.0/myorg/datasets/\"+row['DatasetID']+\"/directQueryRefreshSchedule\"\n",
    "            refreshUrl_response = requests.get(refreshUrl,headers=headers)\n",
    "            if refreshUrl_response.status_code == 200:\n",
    "                refreshJsonstr = refreshUrl_response.content\n",
    "                refreshData = json.loads(refreshJsonstr)\n",
    "            else:\n",
    "                Errors.append([\"Error at Dataset_RefreshDetails in \"+row['DatasetID'+\" \"+row['DatasetName']],refreshUrl_response])\n",
    "                if('error' in refreshData):\n",
    "                    print(row['DatasetID'],row['DatasetName'])\n",
    "        #print(refreshData)\n",
    "        if('frequency' in refreshData):\n",
    "            Frequency.append(refreshData['frequency'])\n",
    "        else:\n",
    "            Frequency.append(\"NA\")\n",
    "        if('days' in refreshData):\n",
    "            Days.append(refreshData['days'])\n",
    "        else:\n",
    "            Days.append(\"NA\")\n",
    "        if('times' in refreshData):\n",
    "            Times.append(refreshData['times'])\n",
    "        else:\n",
    "            Times.append(\"NA\")\n",
    "        if('localTimeZoneId' in refreshData):\n",
    "            LocalTimeZoneId.append(refreshData['localTimeZoneId'])\n",
    "        else:\n",
    "            LocalTimeZoneId.append(\"NA\")\n",
    "        if('notifyOption' in refreshData):\n",
    "            NotifyOption.append(refreshData['notifyOption'])\n",
    "        else:\n",
    "            NotifyOption.append(\"NA\")\n",
    "        DatasetID.append(row['DatasetID'])\n",
    "    except:\n",
    "        DatasetID.append(row['DatasetID'])\n",
    "        Frequency.append(\"NA\")\n",
    "        Days.append(\"NA\")\n",
    "        Times.append(\"NA\")\n",
    "        LocalTimeZoneId.append(\"NA\")\n",
    "        NotifyOption.append(\"NA\")\n",
    "        \n",
    "\n",
    "refreshDic = {'DatasetID': DatasetID, 'Frequency': Frequency, 'Days': Days, 'Times': Times, \n",
    "              'LocalTimeZoneId': LocalTimeZoneId, 'NotifyOption': NotifyOption}\n",
    "df_refreshDetails = pd.DataFrame(refreshDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shedule Refreshes of Datasets excel creation\n",
    "df_refreshDetails.to_excel(\"Dataset_RefreshDetails.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Users and gropus of a dataset\n",
    "DatasetUserAccessRight = []\n",
    "Identifier = []\n",
    "PrincipalType = []\n",
    "DatasetID = []\n",
    "WorkspaceID = []\n",
    "\n",
    "\n",
    "for index, row in df_Workspace_Datasets.iterrows():\n",
    "    usersUrl = \"https://api.powerbi.com/v1.0/myorg/groups/\"+row['WorkspaceID']+\"/datasets/\"+row['DatasetID']+\"/users\"\n",
    "    usersUrl_response = requests.get(usersUrl,headers=headers)\n",
    "    if usersUrl_response.status_code == 200:\n",
    "        usersJsonstr = usersUrl_response.content\n",
    "        users = json.loads(usersJsonstr)\n",
    "        try:\n",
    "            usersData = users['value']\n",
    "            for i in range(len(usersData)):\n",
    "                DatasetUserAccessRight.append(usersData[i]['datasetUserAccessRight'])\n",
    "                Identifier.append(usersData[i]['identifier'])\n",
    "                PrincipalType.append(usersData[i]['principalType'])\n",
    "                DatasetID.append(row['DatasetID'])\n",
    "                WorkspaceID.append(row['WorkspaceID'])\n",
    "        except:\n",
    "            print(row['DatasetID'])\n",
    "            DatasetUserAccessRight.append(\"NA\")\n",
    "            Identifier.append(\"NA\")\n",
    "            PrincipalType.append(\"NA\")\n",
    "            DatasetID.append(row['DatasetID'])\n",
    "            WorkspaceID.append(row['WorkspaceID'])\n",
    "    else:\n",
    "        Errors.append([\"Error at Dataset_UsersDetails in \"+row['DatasetID'],usersUrl_response])\n",
    "\n",
    "        \n",
    "usersDict = {'WorkspaceID': WorkspaceID, 'DatasetID': DatasetID, 'DatasetUserAccessRight': DatasetUserAccessRight, \n",
    "            'Identifier':Identifier , 'PrincipalType': PrincipalType}\n",
    "df_usersDetails = pd.DataFrame(usersDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Error at Dataset_UsersDetails in 3563efcb-ffe1-4369-b4bb-467d4d47c48d',\n",
       "  <Response [401]>],\n",
       " ['Error at Dataset_UsersDetails in b07832c6-31c9-4153-825e-207685400d5f',\n",
       "  <Response [401]>]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets user details excel creation\n",
    "df_usersDetails.to_excel(\"Dataset_UsersDetails.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling Gateway Details\n",
    "GatewayId = []\n",
    "GatewayName = []\n",
    "Type = []\n",
    "GatewayAnnotation = []\n",
    "\n",
    "\n",
    "gatewayUrl = \"https://api.powerbi.com/v1.0/myorg/gateways\"\n",
    "gatewayUrl_response = requests.get(gatewayUrl,headers=headers)\n",
    "if gatewayUrl_response.status_code == 200:\n",
    "    gatewayJsonstr = gatewayUrl_response.content\n",
    "    gateways = json.loads(gatewayJsonstr)\n",
    "    gatewayData = gateways['value']\n",
    "    for gateway in gatewayData:\n",
    "        GatewayId.append(gateway['id'])\n",
    "        GatewayName.append(gateway['name'])\n",
    "        Type.append(gateway['type'])\n",
    "        GatewayAnnotation.append(gateway['gatewayAnnotation'])\n",
    "else:\n",
    "    Errors.append([\"Error at GatewayDetails\",gatewayUrl_response])\n",
    "\n",
    "    \n",
    "gatewayDict = {'GatewayID':GatewayId, 'GatewayName':GatewayName, 'Type':Type, 'GatewayAnnotation':GatewayAnnotation}\n",
    "df_gatewayDetails = pd.DataFrame(gatewayDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Error at Dataset_UsersDetails in 3563efcb-ffe1-4369-b4bb-467d4d47c48d',\n",
       "  <Response [401]>],\n",
       " ['Error at Dataset_UsersDetails in b07832c6-31c9-4153-825e-207685400d5f',\n",
       "  <Response [401]>]]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gateway details excel creation\n",
    "df_gatewayDetails.to_excel(\"GatewayDetails.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing all the Power BI Apps data\n",
    "app_id,app_name,workspaceId,lastUpdated,publishedBy=[],[],[],[],[]\n",
    "appsUrl=\"https://api.powerbi.com/v1.0/myorg/apps\"\n",
    "appsUrl_response = requests.get(appsUrl,headers=headers)\n",
    "if appsUrl_response.status_code == 200:\n",
    "    jsonstr = appsUrl_response.content\n",
    "    apps = json.loads(jsonstr)\n",
    "    tempapps = apps['value'] \n",
    "    for i in range(len(tempapps)):\n",
    "        app_id.append(tempapps[i]['id'])\n",
    "        app_name.append(tempapps[i]['name'])\n",
    "        workspaceId.append(tempapps[i]['workspaceId'])\n",
    "        lastUpdated.append(tempapps[i]['lastUpdate'])\n",
    "        publishedBy.append(tempapps[i]['publishedBy'])\n",
    "else:\n",
    "    Errors.append([\"Error at App_Data\",appsUrl_response])\n",
    "Dict = {'AppID':app_id,'AppName':app_name,'WorkspaceId':workspaceId,'LastUpdated':lastUpdated,\n",
    "        'PublishedBy':publishedBy}\n",
    "df_appsData = pd.DataFrame(Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Error at Dataset_UsersDetails in 3563efcb-ffe1-4369-b4bb-467d4d47c48d',\n",
       "  <Response [401]>],\n",
       " ['Error at Dataset_UsersDetails in b07832c6-31c9-4153-825e-207685400d5f',\n",
       "  <Response [401]>]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing all the Power BI Apps data\n",
    "app_id,app_name,workspaceId,lastUpdated,publishedBy=[],[],[],[],[]\n",
    "appsUrl = \"https://api.powerbi.com/v1.0/myorg/apps\"\n",
    "appsUrl_response = requests.get(appsUrl,headers=headers)\n",
    "if appsUrl_response.status_code == 200:\n",
    "    jsonstr = appsUrl_response.content\n",
    "    apps = json.loads(jsonstr)\n",
    "    tempapps = apps['value'] \n",
    "    for i in range(len(tempapps)):\n",
    "        app_id.append(tempapps[i]['id'])\n",
    "        app_name.append(tempapps[i]['name'])\n",
    "        workspaceId.append(tempapps[i]['workspaceId'])\n",
    "        lastUpdated.append(tempapps[i]['lastUpdate'])\n",
    "        publishedBy.append(tempapps[i]['publishedBy'])\n",
    "else:\n",
    "    Errors.append([\"Error at App_Data\",appsUrl_response])\n",
    "Dict = {'AppID':app_id,'AppName':app_name,'WorkspaceId':workspaceId,'LastUpdated':lastUpdated,\n",
    "        'PublishedBy':publishedBy}\n",
    "df_appsData = pd.DataFrame(Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Error at Dataset_UsersDetails in 3563efcb-ffe1-4369-b4bb-467d4d47c48d',\n",
       "  <Response [401]>],\n",
       " ['Error at Dataset_UsersDetails in b07832c6-31c9-4153-825e-207685400d5f',\n",
       "  <Response [401]>]]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#App data excel creation\n",
    "df_appsData.to_excel(\"App_Data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing all the Power BI Apps data\n",
    "org_apps_url=\"https://api.powerbi.com/v1.0/myorg/apps\"\n",
    "org_apps_url_response = requests.get(org_apps_url,headers=headers)\n",
    "#App Fields\n",
    "appName = []\n",
    "workspaceId = []\n",
    "lastUpdated = []\n",
    "publishedBy = []\n",
    "#App Reports Fileds\n",
    "reportID = []\n",
    "reportType = []\n",
    "reportName = []\n",
    "webUrl = []\n",
    "datasetId = []\n",
    "appId = []\n",
    "originalReportObjectId = []\n",
    "users = []\n",
    "subscriptions = []\n",
    "sections = []\n",
    "if org_apps_url_response.status_code == 200:\n",
    "    jsonstr = org_apps_url_response.content\n",
    "    apps = json.loads(jsonstr)\n",
    "    appsData = apps['value']\n",
    "    for app in appsData:\n",
    "        appReportsUrl=\"https://api.powerbi.com/v1.0/myorg/apps/\"+app['id']+\"/reports\"\n",
    "        appReportsUrl_response = requests.get(appReportsUrl,headers=headers)\n",
    "        if appReportsUrl_response.status_code == 200:\n",
    "            appReportsJsonstr = appReportsUrl_response.content\n",
    "            appReports = json.loads(appReportsJsonstr)\n",
    "            appReportsData = appReports['value']\n",
    "            for report in appReportsData:\n",
    "                #App Fields\n",
    "                appName.append(app['name'])\n",
    "                workspaceId.append(app['workspaceId'])\n",
    "                lastUpdated.append(app['lastUpdate'])\n",
    "                publishedBy.append(app['publishedBy'])  \n",
    "                #App Reports Fileds\n",
    "                reportID.append(report['id'])\n",
    "                reportType.append(report['reportType'])\n",
    "                reportName.append(report['name'])\n",
    "                webUrl.append(report['webUrl'])\n",
    "                datasetId.append(report['datasetId'])\n",
    "                appId.append(report['appId'])\n",
    "                originalReportObjectId.append(report['originalReportObjectId'])\n",
    "                users.append(len(report['users']))\n",
    "                subscriptions.append(report['subscriptions'])\n",
    "        else:\n",
    "            Errors.append([\"Error at AppReportsData at \"+app['id'],appReportsUrl_response])\n",
    "else:\n",
    "    Errors.append([\"Error at AppReportsData\",org_apps_url_response])           \n",
    "AppReportsDataDic = {'AppName':appName,'WorkspaceID':workspaceId,'LastUpdated':lastUpdated,'PublishedBy':publishedBy,'ReportID':reportID,\n",
    "                     'ReportType':reportType,'ReportName':reportName,'ReportWebUrl':webUrl,'ReportDatasetID':datasetId,'AppID':appId,\n",
    "                     'OriginalReportObjectID':originalReportObjectId,'UsersCount':users,'Subscriptions':subscriptions,}\n",
    "df_AppReportsDataDic = pd.DataFrame(AppReportsDataDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AppReports data excel creation\n",
    "df_AppReportsDataDic.to_excel(\"AppReportsData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling all the reports from complete Org from My Workspace\n",
    "Reports=[]\n",
    "Report_Id = []\n",
    "reportType = []\n",
    "Report_Name = []\n",
    "webUrl = []\n",
    "Dataset_Id = []\n",
    "appId = []\n",
    "datasetWorkspaceId = []\n",
    "myorg_reports_url=\"https://api.powerbi.com/v1.0/myorg/reports\"\n",
    "myorg_reports_response = requests.get(myorg_reports_url,headers=headers)\n",
    "if myorg_reports_response.status_code == 200:\n",
    "    jsonstr = myorg_reports_response.content\n",
    "    Report = json.loads(jsonstr)\n",
    "    tempReport = Report['value']\n",
    "    \n",
    "    for report in tempReport:\n",
    "        Report_Id.append(report['id'])\n",
    "        reportType.append(report['reportType'])\n",
    "        Report_Name.append(report['name'])\n",
    "        webUrl.append(report['webUrl'])\n",
    "        Dataset_Id.append(report['datasetId'])\n",
    "        try:\n",
    "            appId.append(report['appId'])\n",
    "        except:\n",
    "            appId.append(\"NA\")\n",
    "        try:\n",
    "            datasetWorkspaceId.append(report['datasetWorkspaceId'])\n",
    "        except:\n",
    "            datasetWorkspaceId.append(\"NA\")\n",
    "else:\n",
    "    Errors.append([\"Error at OrgReportsData\",myorg_reports_response])\n",
    "Org_Reports = {'ReportID':Report_Id,'ReportType':reportType,'ReportName':Report_Name,'WebUrl':webUrl,\n",
    "               'DatasetID':Dataset_Id,'AppID':appId,'DatasetWorkspaceID':datasetWorkspaceId}\n",
    "df_Org_Reports=pd.DataFrame(Org_Reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Org Reports Data\n",
    "df_Org_Reports.to_excel(\"OrgReportsData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling all the reports present in a Workspace\n",
    "ReportID = []\n",
    "ReportName = []\n",
    "weburl = []\n",
    "DatasetID = []\n",
    "DatasetWorkspaceID  = []\n",
    "workspaceNames = []\n",
    "weburl=[]\n",
    "reportType = []\n",
    "embedUrl = []\n",
    "isFromPbix = []\n",
    "for index, row in df_workspace_names_ids.iterrows():\n",
    "    workspaceUrl = \"https://api.powerbi.com/v1.0/myorg/groups/\"+row['WorkspaceID']+\"/reports\"\n",
    "    workspaceUrlResponse = requests.get(workspaceUrl,headers=headers)\n",
    "    if workspaceUrlResponse.status_code == 200:\n",
    "        workspaceUrlJsonstr = workspaceUrlResponse.content\n",
    "        Reports = json.loads(workspaceUrlJsonstr)\n",
    "        ReportsData = Reports['value']\n",
    "        #print(ReportsData)\n",
    "        \n",
    "        for report in ReportsData:\n",
    "            #print(report)\n",
    "            ReportID.append(report['id'])\n",
    "            ReportName.append(report['name'])\n",
    "            weburl.append(report['webUrl'])\n",
    "            isFromPbix.append(report['isFromPbix'])\n",
    "            embedUrl.append(report['embedUrl'])\n",
    "            reportType.append(report['reportType'])\n",
    "            workspaceNames.append(row['WorkspaceName'])\n",
    "            if 'datasetId' in report:\n",
    "                DatasetID.append(report['datasetId'])\n",
    "            else:\n",
    "                DatasetID.append(\"No Dataset\")\n",
    "            if 'datasetWorkspaceId' in report:\n",
    "                DatasetWorkspaceID.append(report['datasetWorkspaceId'])\n",
    "            else:\n",
    "                DatasetWorkspaceID.append(\"No datasetWorkspaceId\")\n",
    "    else:\n",
    "        Errors.append([\"workspaceUrlJsonstr\",myorg_reports_response])\n",
    "\n",
    "tempReports = {'WorkspaceName':workspaceNames,'ReportName':ReportName,'ReportID':ReportID,\n",
    "               'DatasetID':DatasetID,'DatasetWorkspaceID':DatasetWorkspaceID, 'ReportType':reportType,\n",
    "                   'EmbedUrl':embedUrl,'IsFromPbix':isFromPbix,'WebUrl':weburl}\n",
    "df_Reports = pd.DataFrame(tempReports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Org Reports Data\n",
    "df_Reports.to_excel(\"ReportsDataBasedOnWorkspace.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling Page of a Report\n",
    "ReportID = []\n",
    "ReportSection = []\n",
    "PageName = []\n",
    "PageOrder = []\n",
    "\n",
    "for index, row in df_Reports.iterrows():\n",
    "    reportPageUrl =  \"https://api.powerbi.com/v1.0/myorg/reports/\"+row['ReportID']+\"/pages\"\n",
    "    reportPageResponse = requests.get(reportPageUrl,headers=headers)\n",
    "    if reportPageResponse.status_code == 200:\n",
    "        reportPageJsonstr = reportPageResponse.content\n",
    "        ReportPages = json.loads(reportPageJsonstr)\n",
    "        ReportPagesData = ReportPages['value']\n",
    "        #print(ReportsData)\n",
    "        \n",
    "        for page in ReportPagesData:\n",
    "            ReportID.append(row['ReportID'])\n",
    "            if 'name' in page:\n",
    "                ReportSection.append(page['name'])\n",
    "            else:\n",
    "                ReportSection.append(\"NA\")\n",
    "            if 'displayName' in page:\n",
    "                PageName.append(page['displayName'])\n",
    "            else:\n",
    "                PageName.append(\"NA\")\n",
    "            if 'order' in page:\n",
    "                PageOrder.append(page['order'])\n",
    "            else:\n",
    "                PageOrder.append('NA')\n",
    "    else:\n",
    "        Errors.append([\"workspaceUrlJsonstr\",myorg_reports_response])\n",
    "\n",
    "tempReportPages = {'ReportID':ReportID,'ReportSection':ReportSection,'PageName':PageName,\n",
    "               'PageOrder':PageOrder}\n",
    "df_ReportPages = pd.DataFrame(tempReportPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Org Reports Data\n",
    "df_ReportPages.to_excel(\"ReportPages.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling all Refresh history of Dataset\n",
    "\n",
    "RequestID = []\n",
    "ID = []\n",
    "RefreshType = []\n",
    "StartTime = []\n",
    "EndTime = []\n",
    "Status = []\n",
    "RefreshAttempts = []\n",
    "DatasetId = []\n",
    "\n",
    "for index, row in df_Workspace_Datasets.iterrows():\n",
    "    refreshHistoryURL = \"https://api.powerbi.com/v1.0/myorg/groups/\"+row['WorkspaceID']+\"/datasets/\"+row['DatasetID']+\"/refreshes\"\n",
    "    refreshHistoryResponse = requests.get(refreshHistoryURL,headers=headers)\n",
    "    if refreshHistoryResponse.status_code == 200:\n",
    "        refreshHistoryJsonstr = refreshHistoryResponse.content\n",
    "        refreshHistory = json.loads(refreshHistoryJsonstr)\n",
    "        refreshHistoryData = refreshHistory['value']\n",
    "        # print(refreshHistoryData)\n",
    "        if(len(refreshHistoryData)!=0):\n",
    "            # print(\"Yes\")\n",
    "            for refreshData in refreshHistoryData:\n",
    "                #print(report)\n",
    "                RequestID.append(refreshData['requestId'])\n",
    "                ID.append(refreshData['id'])\n",
    "                RefreshType.append(refreshData['refreshType'])\n",
    "                StartTime.append(refreshData['startTime'])\n",
    "                EndTime.append(refreshData['endTime'])\n",
    "                Status.append(refreshData['status'])\n",
    "                RefreshAttempts.append(refreshData['refreshAttempts'])\n",
    "                DatasetId.append(row['DatasetID'])\n",
    "        else:\n",
    "            RequestID.append(\"NA\")\n",
    "            ID.append(\"NA\")\n",
    "            RefreshType.append(\"NA\")\n",
    "            StartTime.append(\"NA\")\n",
    "            EndTime.append(\"NA\")\n",
    "            Status.append(\"NA\")\n",
    "            RefreshAttempts.append(\"NA\")\n",
    "            DatasetId.append(row['DatasetID'])\n",
    "    else:\n",
    "        Errors.append([\"DatasetRefreshHistory in\", row['DatasetID']+ \" \" + row['DatasetName'],refreshHistoryResponse])\n",
    "\n",
    "tempRefreshHistroy = {'RequestID':RequestID,'ID':ID,'RefreshType': RefreshType,'StartTime':StartTime,\n",
    "                      'EndTime':EndTime, 'Status':Status, 'RefreshAttempts':RefreshAttempts,'DatasetID':DatasetId}\n",
    "df_RefreshHistroy = pd.DataFrame(tempRefreshHistroy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Error at Dataset_UsersDetails in 3563efcb-ffe1-4369-b4bb-467d4d47c48d',\n",
       "  <Response [401]>],\n",
       " ['Error at Dataset_UsersDetails in b07832c6-31c9-4153-825e-207685400d5f',\n",
       "  <Response [401]>]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Refresh Histroy Excel\n",
    "df_RefreshHistroy.to_excel(\"DatasetRefreshHistory.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stroing All Dashboard details based on Workspace IDs\n",
    "WorkspaceName = []\n",
    "DashboardName = []\n",
    "DashboardId = []\n",
    "IsReadOnly = []\n",
    "WebUrl = []\n",
    "df_Workspace_Report_Dataset = pd.DataFrame(Dict)\n",
    "for index, row in df_workspace_names_ids.iterrows():\n",
    "    dashboardUrl = \"https://api.powerbi.com/v1.0/myorg/groups/\"+row['WorkspaceID']+\"/dashboards\"\n",
    "    dashboardResponse = requests.get(dashboardUrl,headers=headers)\n",
    "    if dashboardResponse.status_code == 200:\n",
    "        dashboardJsonstr = dashboardResponse.content\n",
    "        dashboard = json.loads(dashboardJsonstr)\n",
    "        dashboardData = dashboard['value']\n",
    "        if(len(dashboardData)==0):\n",
    "            WorkspaceName.append(row['WorkspaceName'])\n",
    "            DashboardId.append(\"NA\")\n",
    "            DashboardName.append(\"NA\")\n",
    "            IsReadOnly.append(\"NA\")\n",
    "            WebUrl.append(\"NA\")\n",
    "        else:\n",
    "            WorkspaceName.append(workspaceNames[Id])\n",
    "            DashboardId.append(dashboardData[0]['id'])\n",
    "            DashboardName.append(dashboardData[0]['displayName'])\n",
    "            IsReadOnly.append(dashboardData[0]['isReadOnly'])\n",
    "            WebUrl.append(dashboardData[0]['webUrl'])  \n",
    "    else:\n",
    "        Errors.append(\"Error at dashboards Data\")\n",
    "DashboardsDic = {'WorkspaceName':WorkspaceName,'DashboardName':DashboardName,'DashboardID':DashboardId,'IsReadOnly':IsReadOnly,'WebUrl':WebUrl}\n",
    "Dashboards_df = pd.DataFrame(DashboardsDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dashboards data excel creation\n",
    "Dashboards_df.to_excel(\"Dashboards_Data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling all the locations from GIT everytime we need to create clone \n",
    "folder_path = \"C://Users//AkhilGrandhi//Downloads//PBIADO/Testing\"\n",
    "bim_paths = []\n",
    "pbir_paths = []\n",
    "target_extensions = ['.bim', '.pbir']\n",
    "# Use os.walk to traverse the folder\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        # print(file)\n",
    "        file_path = os.path.join(root, file)\n",
    "        # Check if the file has a .bim extension\n",
    "        if file.endswith('.bim'):\n",
    "            bim_paths.append(file_path)\n",
    "        # Check if the file has a .pbir extension\n",
    "        elif file.endswith('.pbir'):\n",
    "            pbir_paths.append(file_path)\n",
    "# x,y=[],[]\n",
    "#print(len(pbir_paths))\n",
    "# for i in pbir_paths:\n",
    "#     if \"\\\\Reports\" in i:\n",
    "#         pass\n",
    "#     else:\n",
    "#         x.append(i)\n",
    "# for i in bim_paths:\n",
    "#     if \"\\\\Reports\" not in i:\n",
    "#         y.append(i)\n",
    "# #print(len(x),len(y))\n",
    "# bim_paths,pbir_paths = y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles Data\n",
    "\n",
    "# MQuery = []\n",
    "DatasetNames = []\n",
    "WorkspaceNames = []\n",
    "RoleName = []\n",
    "ModelPermission = []\n",
    "TableName = []\n",
    "FilterExpression = []\n",
    "\n",
    "\n",
    "for i in range(len(bim_paths)):\n",
    "    \n",
    "    bim_file = bim_paths[i]\n",
    "\n",
    "    if(bim_paths[i].endswith('.bim') and bim_paths[i].split('\\\\')[-2].endswith('.Dataset')):\n",
    "        DatasetName = bim_paths[i].split('\\\\')[-2][:-8]\n",
    "    elif(bim_paths[i].endswith('.bim') and bim_paths[i].split('\\\\')[-2].endswith('.SemanticModel')):\n",
    "        DatasetName = bim_paths[i].split('\\\\')[-2][:-14]\n",
    "    WorkspaceName = bim_paths[i].split('\\\\')[-3]\n",
    "    # print(bim_file)\n",
    "    # print(\"---->\",DatasetName)\n",
    "    with open(bim_file, 'r', encoding='utf-8') as bim:\n",
    "        bim_data = json.load(bim)\n",
    "\n",
    "    if('roles' in bim_data['model']):\n",
    "        for role in bim_data['model']['roles']:\n",
    "            \n",
    "            if('tablePermissions' in role):\n",
    "                for TE in role['tablePermissions']:\n",
    "                    DatasetNames.append(DatasetName)\n",
    "                    WorkspaceNames.append(WorkspaceName)\n",
    "                    RoleName.append(role['name'])\n",
    "                    ModelPermission.append(role['modelPermission'])\n",
    "                    TableName.append(TE['name'])\n",
    "                    FilterExpression.append(TE['filterExpression'])\n",
    "                else:\n",
    "                    DatasetNames.append(DatasetName)\n",
    "                    WorkspaceNames.append(WorkspaceName)\n",
    "                    RoleName.append(role['name'])\n",
    "                    ModelPermission.append(role['modelPermission'])\n",
    "                    TableName.append('NA')\n",
    "                    FilterExpression.append('NA')\n",
    "    else:\n",
    "        DatasetNames.append(DatasetName)\n",
    "        WorkspaceNames.append(WorkspaceName)\n",
    "        RoleName.append(\"NA\")\n",
    "        ModelPermission.append('NA')\n",
    "        TableName.append('NA')\n",
    "        FilterExpression.append('NA')\n",
    "\n",
    "RoleDataDic = {'DatasetName': DatasetNames, 'WorkspaceName': WorkspaceNames, 'RoleName': RoleName, \n",
    "               'ModelPermission': ModelPermission, 'TableName': TableName, 'FilterExpression': FilterExpression}\n",
    "\n",
    "df_RoleDataDic = pd.DataFrame(RoleDataDic)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roles data excel creation\n",
    "df_RoleDataDic.to_excel(\"Roles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables Data\n",
    "\n",
    "# MQuery = []\n",
    "TableNames = []\n",
    "TableType = []\n",
    "ColumnNames = []\n",
    "ColumnType = []\n",
    "ColumnExpression = []\n",
    "DataTypes = []\n",
    "SourceProviderType = []\n",
    "MeasureNames = []\n",
    "DaxExpressions = []\n",
    "DatasetNames = []\n",
    "WorkspaceNames = []\n",
    "\n",
    "\n",
    "# ADO_WorkspaceNames = ['Cloud','Cloud BA','CommissionAnalytics','CSOps',\n",
    "#                       'Finance', 'FY-Frozen','GenAI','GlobalOps',\n",
    "#                       'HR','Keystone','NLS','PBI Utility','PS','Sales','T&E']\n",
    "\n",
    "# Org_WorkSpaceNames = [\n",
    "#     'Cloud_Adhoc_Model_Prod','Cloud BA_Adhoc_Model_Prod','CommissionAnalytics_Model_Prod','CSOPS_Adhoc_Model_Prod',\n",
    "#     'Finance_Adhoc_Model_Prod','FY_Frozen_Adhoc_Model_Prod','NA','GlobalOps_Adhoc_Model_Prod','HR_Adhoc_Model_Prod',\n",
    "#     'Keystone_Adhoc_Model_Prod','Nls_Adhoc_Model_Prod',\n",
    "#     'Microsoft Fabric Capacity Metrics EDA','PS_Adhoc_Model_Prod',\n",
    "#     'Sales_Adhoc_Model_Prod','T&E-Adhoc_Model_Prod'\n",
    "#     ]\n",
    "\n",
    "for i in range(len(bim_paths)):\n",
    "    \n",
    "    bim_file = bim_paths[i]\n",
    "    #print(bim_paths[i].split('\\\\'))\n",
    "    if(bim_paths[i].endswith('.bim') and bim_paths[i].split('\\\\')[-2].endswith('.Dataset')):\n",
    "        DatasetName = bim_paths[i].split('\\\\')[-2][:-8]\n",
    "    elif(bim_paths[i].endswith('.bim') and bim_paths[i].split('\\\\')[-2].endswith('.SemanticModel')):\n",
    "        DatasetName = bim_paths[i].split('\\\\')[-2][:-14]\n",
    "    #WorkspaceName = Org_WorkSpaceNames[ADO_WorkspaceNames.index(bim_paths[i].split('\\\\')[-4])]\n",
    "    WorkspaceName = bim_paths[i].split('\\\\')[-3]\n",
    "    # print(bim_file)\n",
    "    # print(\"---->\",DatasetName)\n",
    "    with open(bim_file, 'r', encoding='utf-8') as bim:\n",
    "        bim_data = json.load(bim)\n",
    "    if('tables' in bim_data['model']):\n",
    "        for table in bim_data['model']['tables']:\n",
    "            # print(\"Table--->\",table['name'])\n",
    "            if('columns' in table):\n",
    "                maxlen = len(table['columns'])\n",
    "                for column in table['columns']:\n",
    "                    ColumnNames.append(column['name'])\n",
    "                    if('dataType' in column):\n",
    "                        DataTypes.append(column['dataType'])\n",
    "                    else:\n",
    "                        DataTypes.append('NA')\n",
    "                    if('type' in column):\n",
    "                        ColumnType.append(column['type'])\n",
    "                    else:\n",
    "                        ColumnType.append('Column')\n",
    "                    if('expression' in column):\n",
    "                        CC_Exp = ''\n",
    "                        for cc_ex in column['expression']:\n",
    "                            CC_Exp += cc_ex\n",
    "                        ColumnExpression.append(CC_Exp)\n",
    "                    else:\n",
    "                        ColumnExpression.append('NA')\n",
    "                    if('sourceProviderType' in column):\n",
    "                        SourceProviderType.append(column['sourceProviderType'])\n",
    "                    else:\n",
    "                        SourceProviderType.append('NA')\n",
    "                    TableNames.append(table['name'])\n",
    "                    if('mode' in table['partitions'][0]):\n",
    "                        TableType.append(table['partitions'][0]['mode'])\n",
    "                    else:\n",
    "                        TableType.append('NA')\n",
    "                    DatasetNames.append(DatasetName)\n",
    "                    WorkspaceNames.append(WorkspaceName)\n",
    "            if('measures' in table):\n",
    "                if('columns' in table):\n",
    "                    maxlen = max(len(table['columns']),len(table['measures']))\n",
    "                else:\n",
    "                    maxlen = (len(table['measures']))\n",
    "                for measure in table['measures']:\n",
    "                    MeasureNames.append(measure['name'])\n",
    "                    if('expression' in measure):\n",
    "                        daxExp = ''\n",
    "                        for ex in measure['expression']:\n",
    "                            daxExp += ex\n",
    "                        DaxExpressions.append(daxExp)\n",
    "                    else:\n",
    "                        DaxExpressions.append('NA')\n",
    "                \n",
    "                if('columns' in table):\n",
    "                    if(len(table['columns'])>len(table['measures'])):\n",
    "                        diff = len(table['columns'])-len(table['measures']) \n",
    "                        MeasureNames += ['NA']*diff\n",
    "                        DaxExpressions += ['NA']*diff\n",
    "\n",
    "                    elif(len(table['columns'])<len(table['measures'])):\n",
    "                        diff = len(table['measures'])-len(table['columns'])\n",
    "                        TableNames += [table['name']]*diff\n",
    "                        if('mode' in table['partitions'][0]):\n",
    "                            TableType += [table['partitions'][0]['mode']]*diff\n",
    "                        else:\n",
    "                            TableType += ['NA']*diff\n",
    "                        ColumnNames += ['NA']*diff\n",
    "                        ColumnType += ['NA']*diff\n",
    "                        ColumnExpression += ['NA']*diff\n",
    "                        DataTypes += ['NA']*diff\n",
    "                        SourceProviderType += ['NA']*diff\n",
    "                        DatasetNames += [DatasetName]*diff\n",
    "                        WorkspaceNames += [WorkspaceName]*diff\n",
    "                        \n",
    "                    elif(len(table['columns'])==len(table['measures'])):\n",
    "                        continue \n",
    "                if('measures' in table and 'columns' not in table):\n",
    "                    TableNames += [table['name']]*maxlen\n",
    "                    if('mode' in table['partitions'][0]):\n",
    "                        TableType += [table['partitions'][0]['mode']]*maxlen\n",
    "                    else:\n",
    "                        TableType += ['NA']*maxlen\n",
    "                    ColumnNames += ['NA']*maxlen\n",
    "                    ColumnType += ['NA']*maxlen\n",
    "                    ColumnExpression += ['NA']*maxlen\n",
    "                    DataTypes += ['NA']*maxlen\n",
    "                    SourceProviderType += ['NA']*maxlen\n",
    "                    DatasetNames += [DatasetName]*maxlen\n",
    "                    WorkspaceNames += [WorkspaceName]*maxlen\n",
    "            else:\n",
    "                MeasureNames += ['NA']*maxlen\n",
    "                DaxExpressions += ['NA']*maxlen\n",
    "    else:\n",
    "        TableNames += [\"No Tables\"]\n",
    "        DatasetNames += [DatasetName]\n",
    "        WorkspaceNames += [WorkspaceName]\n",
    "        TableType += ['NA']\n",
    "        ColumnNames += ['NA']\n",
    "        ColumnType += ['NA']\n",
    "        ColumnExpression += ['NA']\n",
    "        DataTypes += ['NA']\n",
    "        SourceProviderType += ['NA']\n",
    "             \n",
    "        \n",
    "                       \n",
    "data_dict = {\n",
    "'TableName': TableNames,\n",
    "'TableType': TableType,\n",
    "'ColumnName': ColumnNames,\n",
    "'ColumnType': ColumnType,\n",
    "'ColumnExpression': ColumnExpression,\n",
    "'DataTypes': DataTypes,\n",
    "'SourceProviderType': SourceProviderType,\n",
    "'MeasureName': MeasureNames,\n",
    "'DaxExpression': DaxExpressions,\n",
    "'DatasetName': DatasetNames,\n",
    "'WorkspaceName': WorkspaceNames \n",
    "}\n",
    "df_tablesData = pd.DataFrame.from_dict(data_dict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tablesData.to_excel(\"ModelMetaData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationships Data\n",
    "relationship_data=[]\n",
    "fromColumn = []\n",
    "fromTable = []\n",
    "toColumn = []\n",
    "toTable = []\n",
    "mainDatasetList = []\n",
    "mainWorkspaceList = []\n",
    "crossFilteringBehavior = []\n",
    "toCardinality = []\n",
    "securityFilteringBehavior = []\n",
    "for i in range(len(bim_paths)):\n",
    "    bim_file = bim_paths[i]\n",
    "    #print(bim_paths[i].split('\\\\'))\n",
    "    if(bim_paths[i].endswith('.bim') and bim_paths[i].split('\\\\')[-2].endswith('.Dataset')):\n",
    "        DatasetName = bim_paths[i].split('\\\\')[-2][:-8]\n",
    "    elif(bim_paths[i].endswith('.bim') and bim_paths[i].split('\\\\')[-2].endswith('.SemanticModel')):\n",
    "        DatasetName = bim_paths[i].split('\\\\')[-2][:-14]\n",
    "    WorkspaceName = bim_paths[i].split('\\\\')[-3]\n",
    "    try:\n",
    "        for relationship in bim_data['model']['relationships']:\n",
    "            mainWorkspaceList.append(WorkspaceName)\n",
    "            mainDatasetList.append(DatasetName)\n",
    "            fromColumn.append(relationship['fromColumn'])\n",
    "            fromTable.append(relationship['fromTable'])\n",
    "            toColumn.append(relationship['toColumn'])\n",
    "            toTable.append(relationship['toTable'])\n",
    "            if('crossFilteringBehavior' in relationship):\n",
    "                crossFilteringBehavior.append(relationship['crossFilteringBehavior'])\n",
    "            else:\n",
    "                crossFilteringBehavior.append('oneDirection')\n",
    "            if('toCardinality' in relationship):\n",
    "                toCardinality.append(relationship['toCardinality'])\n",
    "            else:\n",
    "                toCardinality.append('one')\n",
    "            if('securityFilteringBehavior' in relationship):\n",
    "                securityFilteringBehavior.append(relationship['securityFilteringBehavior'])\n",
    "            else:\n",
    "                securityFilteringBehavior.append('NA')\n",
    "    except Exception as error:\n",
    "        # print(DatasetName,error)\n",
    "        # print(relationship)\n",
    "        mainWorkspaceList.append(WorkspaceName)\n",
    "        mainDatasetList.append(DatasetName)\n",
    "        fromColumn.append(\"NA\")\n",
    "        fromTable.append(\"NA\")\n",
    "        toColumn.append(\"NA\")\n",
    "        toTable.append(\"NA\")\n",
    "        crossFilteringBehavior.append(\"NA\")\n",
    "        toCardinality.append(\"NA\")\n",
    "        \n",
    "data_dict = {\n",
    "        'Workspacename': mainWorkspaceList,\n",
    "        'DatasetName': mainDatasetList, \n",
    "        'From Column': fromColumn, \n",
    "        'From Table': fromTable,\n",
    "        'To Column': toColumn,\n",
    "        'To Table': toTable,\n",
    "        'CrossFilteringBehavior': crossFilteringBehavior,\n",
    "        'ToCardinality': toCardinality,\n",
    "        'SecurityFilteringBehavior': securityFilteringBehavior\n",
    "    }\n",
    "\n",
    "df_relationshipsdata = pd.DataFrame.from_dict(data_dict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationships data excel creation\n",
    "df_relationshipsdata.to_excel(\"RelationshipsData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Identifier = []\n",
    "UserPrincipalName = []\n",
    "DisplayName = []\n",
    "\n",
    "#Azure App Details\n",
    "TENANT_ID = ''\n",
    "CLIENT_ID = ''\n",
    "CLIENT_SECRET = ''\n",
    "GROUP_ID = '6c287664-023b-4dcc-aa32-6532818d2fa3'  # Your group Object ID\n",
    "url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
    "\n",
    "Azureheaders = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "data = {\n",
    "    \"grant_type\": \"client_credentials\",\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"client_secret\": CLIENT_SECRET,\n",
    "    \"scope\": \"https://graph.microsoft.com/.default\"\n",
    "}\n",
    "\n",
    "IdentifierResponse = requests.post(url, headers=Azureheaders, data=data)\n",
    "IdentifierResponse.raise_for_status()  # Raise an error for bad status\n",
    "# print(response.json().get(\"access_token\"))\n",
    "access_token = IdentifierResponse.json().get(\"access_token\")\n",
    "\n",
    "uniqueGroupIdentifiers = list(set(df_usersDetails[df_usersDetails['PrincipalType'] == 'Group']['Identifier']))\n",
    "\n",
    "for identifier in uniqueGroupIdentifiers:\n",
    "    # print(identifier)\n",
    "    groupMembersUrl = \"https://graph.microsoft.com/v1.0/groups/\"+identifier+\"/members\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "    # print(groupMembersResponse)\n",
    "    groupMembersResponse = requests.get(groupMembersUrl, headers=headers)\n",
    "    if(groupMembersResponse.status_code == 200):\n",
    "        members = groupMembersResponse.json().get(\"value\", [])\n",
    "    \n",
    "    for member in members:\n",
    "        Identifier.append(identifier)\n",
    "        UserPrincipalName.append(member['userPrincipalName'])\n",
    "        DisplayName.append(member['displayName'])\n",
    "\n",
    "\n",
    "groupMembersDict = {'Identifier': Identifier, 'UserPrincipalName': UserPrincipalName, 'DisplayName': DisplayName}\n",
    "\n",
    "df_groupMembers = pd.DataFrame(groupMembersDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets user details excel creation\n",
    "df_groupMembers.to_excel(\"IdentifierMembers.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "Identifier = []\n",
    "IdentifierName = []\n",
    "CreatedDateTime = []\n",
    "Mail = []\n",
    "for identifier in uniqueGroupIdentifiers:\n",
    "    groupDetailsUrl = \"https://graph.microsoft.com/v1.0/groups/\"+identifier\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "    groupDetailsResponse = requests.get(groupDetailsUrl, headers=headers)\n",
    "    if(groupDetailsResponse.status_code == 200):\n",
    "        groupJsonstr = groupDetailsResponse.content\n",
    "        groupData = json.loads(groupJsonstr)\n",
    "        Identifier.append(groupData['id'])\n",
    "        IdentifierName.append(groupData['displayName'])\n",
    "        CreatedDateTime.append(groupData['createdDateTime'])\n",
    "        Mail.append(groupData['mail'])\n",
    "\n",
    "groupDataDic = {'Identifier': Identifier, 'IdentifierName': IdentifierName, 'CreatedDateTime': CreatedDateTime, 'Mail': Mail}\n",
    "df_groupData = pd.DataFrame(groupDataDic)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets user details excel creation\n",
    "df_groupData.to_excel(\"IdentifierDetails.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
